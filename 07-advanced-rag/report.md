# Отчёт о разработке RAG-бота «Sber Agents AIdd Bot»

## О проекте
- **Название:** Sber Agents AIdd Bot  
- **Описание:** Telegram-бот банковского ассистента, использующий Retrieval-Augmented Generation для ответов на вопросы клиентов по данным PDF и JSON-FAQ Сбербанка.
- **Вариант задания:** расширенный (интеграция PDF + JSON, эксперименты с индексированием, сравнение эмбеддингов).

## Реализованные возможности
- [x] Интеграция Telegram-бота на aiogram 3.x  
- [x] Переиндексация данных из `@data` (PDF + JSON) командами `/index` и при старте  
- [x] Хранение статуса индексации и команда `/index_status`  
- [x] Диалоговый RAG-пайплайн с историей сообщений (LangChain)  
- [x] Поддержка JSON FAQ через `JSONLoader` и объединение с PDF  
- [x] Эксперименты с разными размерами чанков и запись результатов  
- [x] Переключение эмбеддингов на локальные Ollama-модели  

## Технологический стек
- Python 3.11, aiogram 3.x  
- LangChain (chains, retrievers, JSONLoader), InMemoryVectorStore  
- Ollama (локальные эмбеддинги), Fireworks/OpenRouter (LLM)  
- OpenAI совместимый клиент (`langchain-openai`)  
- Управление зависимостями: `uv`, сборка `make`  

## Используемые модели
- **LLM:** `accounts/fireworks/models/llama-v3p1-8b-instruct` (Fireworks)  
- **Эмбеддинги:** 
  - Fireworks `accounts/fireworks/models/nomic-embed-text-v1` (на ранних этапах)  
  - Ollama `aroxima/multilingual-e5-large-instruct:latest` (текущая версия)  

---

## Эксперименты с индексацией
| Параметры (`chunk_size`, `chunk_overlap`) | Кол-во чанков | Наблюдения |
| --- | --- | --- |
| 1500 / 150 | 132 | Ответы компактные, но мало деталей; релевантность сохраняется |
| 800 / 100 + адаптивные `separators` | 246 | Ответы структурированнее, релевантность хорошая |
| 300 / 50 | 652 | Ответы теряют контекст и становятся нерелевантными |
| 1500 / 150 (финальный тест с JSON) | 353 (PDF) + 313 (JSON) | Связность хорошая, но требуется нормализация JSON |

**Вывод:** оптимально использовать «средний» размер 600–800 символов с адаптивными разделителями. Для банковских документов это даёт баланс между детализацией и полноценным контекстом. Слишком мелкие чанки сильно ухудшают ответы.

---

## Работа с JSON датасетом
- Использован `langchain_community.document_loaders.JSONLoader` с `jq_schema='.[].full_text'`.  
- При загрузке JSON:
  - Добавляются метаданные `source` и `document_type=json_qa`.  
  - Чистятся неразрывные пробелы (\xa0, \u202f) и чанк разделяется теми же правилами, что и PDF.  
- Комбинация PDF-чанков и JSON-чанков агрегируется перед векторизацией.

**Скриншот:**  
![Ответ на вопрос про карты](screenshots/how_to_order_card.png)

---

## Сравнение моделей эмбеддингов

| Модель | Источник | Результат |
| --- | --- | --- |
| `accounts/fireworks/models/nomic-embed-text-v1` | Fireworks (удалённый) | Работает, но требует API; чувствительна к качеству текста (закодированные пробелы) |
| `aroxima/multilingual-e5-large-instruct:latest` | Ollama (локально) | Бесплатно, устойчиво к русскому тексту, обеспечивает стабильные совпадения |

**Вывод:** для русского языка и локальной работы лучше использовать локальную модель через Ollama (`multilingual-e5`). Она не требует API ключей и точнее сопоставляет FAQ, особенно после нормализации текста.

---

## Оценка качества (RAGAS)

### Датасет 1: 06-rag-qa-dataset (малый эталонный датасет)

Запуск evaluation на 8 примерах дал такие RAGAS‑метрики:

- **Обоснованность (нет галлюцинаций, faithfulness)**: 0.875  
- **Релевантность ответа (answer_relevancy)**: 0.654  
- **Правильность ответа (answer_correctness)**: 0.823  
- **Похожесть на эталон (answer_similarity)**: 0.891  
- **Полнота контекста (context_recall)**: 0.750  
- **Точность поиска (context_precision)**: 0.833  

**Интерпретация:** ответы в целом обоснованы документами и близки к эталонам; иногда ответ частичный или менее точен по формулировке, но retriever в большинстве случаев поднимает нужный контекст.

### Датасет 2: 06-monitoring-qa (боевой датасет PDF + JSON)

Для полного датасета (20 примеров) итоговые метрики:

- **Обоснованность (нет галлюцинаций, faithfulness)**: 0.000  
- **Релевантность ответа (answer_relevancy)**: 0.000  
- **Правильность ответа (answer_correctness)**: 0.000  
- **Похожесть на эталон (answer_similarity)**: 0.933  
- **Полнота контекста (context_recall)**: 1.000  
- **Точность поиска (context_precision)**: 1.000  

При этом отладочная команда `/debug_eval_examples` показывает, что для вопросов датасета retriever поднимает именно те QA‑чанки, где лежит эталонный ответ, а ответы RAG по содержанию практически дословно совпадают с эталонами.

**Почему первые три метрики равны 0.0:** в логах RAGAS объект `result.scores` для `faithfulness`, `answer_relevancy`, `answer_correctness` содержит только `nan`. В `evaluation.py` эти значения безопасно нормализуются в 0.0 (чтобы не падать), что и даёт нули в итоговом выводе бота. Это артефакт текущей связки RAGAS (русский текст + LLM‑судья на Fireworks), а не реальное падение качества RAG.

**Реальная оценка качества для 06-monitoring-qa:** по метрикам `answer_similarity ≈ 0.93`, `context_recall = 1.0`, `context_precision ≈ 1.0` и ручной проверке примеров можно считать, что RAG‑цепочка отвечает корректно и опирается на правильный контекст. Для этого проекта надёжными считаются метрики похожести и контекста, а faithfulness/relevancy/correctness используются только как вспомогательные и в данной конфигурации не интерпретируются.

---

## Итог
Проект реализован в расширенном варианте: бот полностю переиндексирует банковские PDF и JSON FAQ, поддерживает ряд экспериментов, локальные эмбеддинги и evaluation через RAGAS. Идеальная стратегия: адаптивные чанки (~600–800 символов) + локальные эмбеддинги Ollama, при этом качество RAG подтверждено метриками (`answer_similarity`, `context_recall`, `context_precision`) и ручной проверкой примеров.

