# ============================================
# REQUIRED environment variables
# ============================================
# These must be filled in for the bot to work

# Telegram Bot Token (get from @BotFather)
TELEGRAM_BOT_TOKEN=your_telegram_bot_token_here

# API key for LLM (OpenAI, Fireworks, OpenRouter, etc.)
OPENAI_API_KEY=your_openai_api_key_here

# ============================================
# OPTIONAL environment variables (with defaults)
# ============================================
# These have default values and can be left as-is or customized

# Base URL for API (Fireworks, OpenAI, OpenRouter, etc.)
# Default: https://api.fireworks.ai/inference/v1
OPENAI_BASE_URL=https://api.fireworks.ai/inference/v1

# Model for answer generation
# Default: accounts/fireworks/models/llama-3.1-8b-instruct
# Популярные модели Fireworks (проверьте актуальность на https://fireworks.ai/models):
#   accounts/fireworks/models/llama-3.1-8b-instruct (рекомендуется)
#   accounts/fireworks/models/llama-v3-70b-instruct (лучше качество)
#   accounts/fireworks/models/llama-v3-8b-instruct (быстрая)
# ============================================
# БЕСПЛАТНЫЕ АЛЬТЕРНАТИВЫ LLM API
# ============================================

# === Вариант 1: OpenRouter (рекомендуется для бесплатного использования) ===
# Регистрация: https://openrouter.ai/
# Получение API ключа: https://openrouter.ai/keys
# Бесплатные модели (с суффиксом :free):
#   meta-llama/llama-3.1-8b-instruct:free (рекомендуется)
#   google/gemma-2-2b-it:free
#   microsoft/phi-3-mini-128k-instruct:free
#   qwen/qwen-2-7b-instruct:free
# Настройка:
#   LLM_PROVIDER=openai
#   OPENAI_BASE_URL=https://openrouter.ai/api/v1
#   LLM_MODEL=meta-llama/llama-3.1-8b-instruct:free
#   OPENAI_API_KEY=your_openrouter_api_key_here
#   RAGAS_LLM_MODEL=meta-llama/llama-3.1-8b-instruct:free
# Примечание:
#   - OpenAI-совместимый API (легкая интеграция)
#   - Работает в России (может потребоваться VPN для регистрации)
#   - Есть бесплатные модели
#   - Подробная инструкция: docs/openrouter-setup.md

# === Вариант 2: Groq (быстро, бесплатно) ===
# Регистрация: https://console.groq.com/
# Бесплатные модели (проверьте актуальность на https://console.groq.com/docs/models):
#   openai/gpt-oss-20b (рекомендуется - OpenAI модель с открытыми весами)
#   llama-3.1-8b-instant
#   llama-3.1-70b-versatile
#   mixtral-8x7b-32768
# Настройка:
#   OPENAI_BASE_URL=https://api.groq.com/openai/v1
#   LLM_MODEL=openai/gpt-oss-20b
#   OPENAI_API_KEY=your_groq_api_key_here

# === Вариант 3: Together AI (бесплатный tier) ===
# Регистрация: https://together.ai/
# Настройка:
#   OPENAI_BASE_URL=https://api.together.xyz/v1
#   LLM_MODEL=meta-llama/Llama-3-8b-chat-hf
#   OPENAI_API_KEY=your_together_api_key_here

# === Вариант 4: Hugging Face Inference API (бесплатно) ===
# Регистрация: https://huggingface.co/
# Получение токена: https://huggingface.co/settings/tokens
# ВАЖНО: Используйте новый endpoint router.huggingface.co (старый api-inference больше не работает)
# Бесплатные модели (проверьте актуальность на https://huggingface.co/models):
#   meta-llama/Meta-Llama-3-8B-Instruct
#   mistralai/Mistral-7B-Instruct-v0.2
#   microsoft/Phi-3-mini-4k-instruct
#   google/gemma-2-2b-it
# Настройка:
#   OPENAI_BASE_URL=https://router.huggingface.co/v1
#   LLM_MODEL=meta-llama/Meta-Llama-3-8B-Instruct
#   OPENAI_API_KEY=your_huggingface_token_here
# Примечание: 
#   - Может быть медленнее, так как модели запускаются по запросу (cold start)
#   - Бесплатный tier: до 30,000 запросов в месяц
#   - Первый запрос может занять 10-30 секунд для загрузки модели

# === Вариант 5: DeepInfra (бесплатный tier) ===
# Регистрация: https://deepinfra.com/
# Бесплатные модели:
#   meta-llama/Meta-Llama-3-8B-Instruct
#   mistralai/Mistral-7B-Instruct-v0.2
#   microsoft/Phi-3-mini-4k-instruct
# Настройка:
#   OPENAI_BASE_URL=https://api.deepinfra.com/v1/openai
#   LLM_MODEL=meta-llama/Meta-Llama-3-8B-Instruct
#   OPENAI_API_KEY=your_deepinfra_api_key_here

# === Вариант 6: Perplexity AI (бесплатный tier) ===
# Регистрация: https://www.perplexity.ai/
# Настройка:
#   OPENAI_BASE_URL=https://api.perplexity.ai
#   LLM_MODEL=llama-3.1-sonar-small-128k-online
#   OPENAI_API_KEY=your_perplexity_api_key_here
# Примечание: Ограниченная бесплатная квота

# === Вариант 7: Yandex GPT (работает в России) ===
# Регистрация: https://yandex.cloud/
# Документация: https://yandex.cloud/docs/foundation-models/concepts/
# Получение API ключа: https://console.cloud.yandex.ru/folders/<folder-id>/foundation-models
# Модели:
#   yandexgpt (рекомендуется)
#   yandexgpt-lite (быстрая)
# Настройка:
#   OPENAI_BASE_URL=https://llm.api.cloud.yandex.net/foundationModels/v1/completion
#   LLM_MODEL=yandexgpt
#   OPENAI_API_KEY=your_yandex_api_key_here
# Примечание:
#   - Требуется регистрация в Yandex Cloud
#   - Есть бесплатный tier
#   - Оптимизирована для русского языка

# === Вариант 8: GigaChat (работает в России, от Сбера) ===
# Регистрация: https://developers.sber.ru/gigachat
# Документация: https://developers.sber.ru/docs/ru/gigachat/api/reference
# Получение API ключа: https://developers.sber.ru/portal/products/gigachat
# Модели:
#   GigaChat (основная модель)
# Настройка:
#   LLM_PROVIDER=gigachat
#   OPENAI_API_KEY=your_gigachat_authorization_key_here
#   LLM_MODEL=GigaChat
#   RAGAS_LLM_MODEL=GigaChat
# Примечание:
#   - Требуется регистрация в Sber ID
#   - Есть бесплатный tier
#   - Оптимизирована для русского языка
#   - ВАЖНО: Используйте LLM_PROVIDER=gigachat для активации поддержки GigaChat
#   - Подробная инструкция: docs/gigachat-setup.md

# === Вариант 9: OpenGate (работает в России, OpenAI-совместимый, РЕКОМЕНДУЕТСЯ) ===
# Регистрация: https://opengatellm.ru/
# Каталог моделей: https://opengatellm.ru/catalog.html
# Настройка:
#   LLM_PROVIDER=openai
#   OPENAI_BASE_URL=https://api.opengatellm.ru/v1
#   LLM_MODEL=llama-3.1-8b-instruct (или другая доступная модель)
#   OPENAI_API_KEY=your_opengate_api_key_here
#   RAGAS_LLM_MODEL=llama-3.1-8b-instruct
# Примечание:
#   - OpenAI-совместимый API (легкая интеграция)
#   - Работает без VPN
#   - Доступ к различным моделям (Gemini, Grok, DeepSeek, Qwen, Mistral, Llama)
#   - Подробная инструкция: docs/opengate-setup.md

# === Вариант 10: LLMost (работает в России, OpenAI-совместимый) ===
# Регистрация: https://llmost.ru/
# Документация: https://llmost.ru/docs
# Настройка:
#   LLM_PROVIDER=openai
#   OPENAI_BASE_URL=https://llmost.ru/api/v1
#   LLM_MODEL=x-ai/grok-4.1-fast (или другая доступная модель - проверьте на сайте)
#   OPENAI_API_KEY=your_llmost_api_key_here
# Примечание:
#   - OpenAI-совместимый API
#   - Работает без VPN
#   - Есть бесплатный tier
#
# === Использование LLMost для RAGAS evaluation (рекомендуется при использовании Groq) ===
# Если основной бот использует Groq, но Groq имеет строгие rate limits для evaluation,
# можно использовать LLMost только для RAGAS:
#   # Основной бот - Groq
#   LLM_MODEL=openai/gpt-oss-20b
#   OPENAI_BASE_URL=https://api.groq.com/openai/v1
#   OPENAI_API_KEY=your_groq_api_key_here
#   # RAGAS evaluation - LLMost (чтобы не бить лимиты Groq)
#   RAGAS_LLM_MODEL=x-ai/grok-4.1-fast
#   RAGAS_OPENAI_BASE_URL=https://llmost.ru/api/v1
#   RAGAS_OPENAI_API_KEY=your_llmost_api_key_here
# Примечание:
#   - Подробная инструкция: RAGAS_LLMOST_SETUP.md

# === Вариант 10: Ollama (локально, полностью бесплатно) ===
# Установка: https://ollama.ai/
# Запуск локально:
#   ollama pull llama3.1:8b
# Настройка:
#   OPENAI_BASE_URL=http://localhost:11434/v1
#   LLM_MODEL=llama3.1:8b
#   OPENAI_API_KEY=ollama  # можно любой, не используется
# Примечание: Требует установки Ollama и достаточных ресурсов (RAM/GPU)

# === Вариант 8: Google Gemini (бесплатный tier) ===
# Регистрация: https://aistudio.google.com/
# Настройка:
#   OPENAI_BASE_URL=https://generativelanguage.googleapis.com/v1beta
#   LLM_MODEL=gemini-pro
#   OPENAI_API_KEY=your_google_api_key_here
# Примечание: Может потребоваться адаптация кода для Gemini API
LLM_MODEL=accounts/fireworks/models/llama-v3-8b-instruct

# Embeddings model
# Default: accounts/fireworks/models/nomic-embed-text-v1
# Popular models:
#   OpenAI/Fireworks: accounts/fireworks/models/nomic-embed-text-v1
#   HuggingFace (multilingual): sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
#   HuggingFace (Russian): cointegrated/rubert-tiny2, ai-forever/sbert_large_nlu_ru
#   HuggingFace (high quality): intfloat/multilingual-e5-base, intfloat/multilingual-e5-large
#   Ollama: nomic-embed-text
EMBEDDINGS_MODEL=accounts/fireworks/models/nomic-embed-text-v1

# Embeddings provider for main system (openai/huggingface/ollama)
# Default: ollama
# openai - uses OpenAI/Fireworks API (requires API key, paid)
# huggingface - uses local HuggingFace models (free, runs locally, requires internet for first download)
# ollama - uses Ollama (requires local Ollama server, free)
EMBEDDINGS_PROVIDER=ollama

# System role for the bot
# Default: банковский ассистент
SYSTEM_ROLE=банковский ассистент

# Number of recent turns for context
# Default: 8
CONTEXT_TURNS=8

# Number of chunks to retrieve (top-K)
# Default: 4
RETRIEVER_K=4

# Path to data directory (PDF and JSON files)
# Default: @data
DATA_PATH=@data

# Logging level (DEBUG, INFO, WARNING, ERROR)
# Default: INFO
LOG_LEVEL=INFO

# Show sources in answer (true/false)
# Default: false
SHOW_SOURCES=false

# ============================================
# Advanced RAG settings (optional)
# ============================================

# RAG pipeline mode (semantic/hybrid/hybrid+reranker)
# Default: semantic
# semantic - only semantic search using embeddings
# hybrid - combination of semantic search and BM25 (keyword-based search)
# hybrid+reranker - hybrid retrieval with additional ranking via Cross-Encoder
RAG_MODE=semantic

# Number of documents for semantic retrieval (top-K by semantic similarity)
# Default: 4
SEMANTIC_K=4

# Number of documents for BM25 retrieval (top-K by keyword-based search)
# Default: 4
BM25_K=4

# Final number of documents after combining semantic and BM25 in hybrid mode
# Default: 4
HYBRID_K=4

# Final number of documents after reranking in hybrid+reranker mode
# Default: 4
RERANKER_K=4

# Cross-Encoder model for reranking
# Default: CrossEncoder/ms-marco-MiniLM-L-6-v2
# Popular models:
#   CrossEncoder/ms-marco-MiniLM-L-6-v2 (fast, good quality)
#   CrossEncoder/ms-marco-MiniLM-L-12-v2 (better quality, slower)
CROSSENCODER_MODEL=CrossEncoder/ms-marco-MiniLM-L-6-v2

# Cross-Encoder provider (huggingface)
# Default: huggingface
# Only huggingface is supported (models from sentence-transformers)
CROSSENCODER_PROVIDER=huggingface

# ============================================
# LangSmith settings (optional)
# ============================================

# LangSmith API key (get from smith.langchain.com)
# Starts with lsv2_pt_...
# Automatically enables tracing of all RAG pipeline requests when present
LANGSMITH_API_KEY=your_langsmith_api_key_here

# Project name in LangSmith (optional)
LANGSMITH_PROJECT=06-monitoring-qa

# ============================================
# RAGAS settings (optional, for evaluation)
# ============================================

# Model for RAGAS metrics (default: LLM_MODEL)
RAGAS_LLM_MODEL=

# Embeddings model for RAGAS (default: EMBEDDINGS_MODEL)
# Leave empty to use EMBEDDINGS_MODEL
# Popular HuggingFace models for RAGAS:
#   sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 (fast, good quality)
#   intfloat/multilingual-e5-base (better quality, slower)
#   cointegrated/rubert-tiny2 (Russian, very fast)
RAGAS_EMBEDDING_MODEL=

# Embeddings provider for RAGAS (openai/huggingface)
# Default: "openai"
# For free usage, you can use "huggingface" with local models
# Note: RAGAS evaluation requires embeddings, so set this to match your setup
RAGAS_EMBEDDINGS_PROVIDER=openai

# ============================================
# HuggingFace settings (optional, for local models)
# ============================================

# Device for HuggingFace models (cpu/cuda)
# Default: cpu
# Use "cuda" if you have GPU available and want to use it
HUGGINGFACE_DEVICE=cpu

# Cache folder for HuggingFace models (optional)
# Default: None (uses default HuggingFace cache)
# Example: ./models/cache
HUGGINGFACE_CACHE_FOLDER=

# Normalize embeddings for HuggingFace (true/false)
# Default: true
# Normalizing embeddings improves cosine similarity calculations
# Recommended: true for most use cases
HUGGINGFACE_NORMALIZE_EMBEDDINGS=true

# ============================================
# Evaluation settings (optional)
# ============================================

# Maximum concurrent requests during evaluation
# Default: 1 (sequential processing, recommended for Groq API with 6000 TPM limit)
# Increase to 2-5 for other providers with higher rate limits
# Lower values = slower but more reliable, fewer rate limit errors
EVALUATION_MAX_CONCURRENT=1

# Delay between requests during evaluation (seconds)
# Default: 2.0 (recommended for Groq API)
# Increase to 3.0-5.0 if you still encounter rate limit errors
# Decrease to 0.5-1.0 for providers with higher rate limits
EVALUATION_DELAY_BETWEEN_REQUESTS=2.0

# Maximum number of examples to process (0 = no limit)
# Useful for testing with a smaller subset
EVALUATION_MAX_EXAMPLES=0

# ============================================
# Example configurations for different scenarios
# ============================================
# Uncomment and modify one of the following configurations based on your needs

# === Configuration 1: OpenAI/Fireworks API (requires API key) ===
# EMBEDDINGS_PROVIDER=openai
# EMBEDDINGS_MODEL=accounts/fireworks/models/nomic-embed-text-v1
# RAGAS_EMBEDDINGS_PROVIDER=openai
# RAGAS_EMBEDDING_MODEL=accounts/fireworks/models/nomic-embed-text-v1

# === Configuration 2: HuggingFace (free, local models) ===
# EMBEDDINGS_PROVIDER=huggingface
# EMBEDDINGS_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# RAGAS_EMBEDDINGS_PROVIDER=huggingface
# RAGAS_EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# HUGGINGFACE_DEVICE=cpu
# HUGGINGFACE_NORMALIZE_EMBEDDINGS=true

# === Configuration 3: HuggingFace with Russian model (best for Russian language) ===
# EMBEDDINGS_PROVIDER=huggingface
# EMBEDDINGS_MODEL=cointegrated/rubert-tiny2
# RAGAS_EMBEDDINGS_PROVIDER=huggingface
# RAGAS_EMBEDDING_MODEL=cointegrated/rubert-tiny2
# HUGGINGFACE_DEVICE=cpu
# HUGGINGFACE_NORMALIZE_EMBEDDINGS=true

# === Configuration 4: HuggingFace with high-quality model (slower but better) ===
# EMBEDDINGS_PROVIDER=huggingface
# EMBEDDINGS_MODEL=intfloat/multilingual-e5-base
# RAGAS_EMBEDDINGS_PROVIDER=huggingface
# RAGAS_EMBEDDING_MODEL=intfloat/multilingual-e5-base
# HUGGINGFACE_DEVICE=cpu
# HUGGINGFACE_NORMALIZE_EMBEDDINGS=true

# === Configuration 5: Ollama (requires local Ollama server) ===
# EMBEDDINGS_PROVIDER=ollama
# EMBEDDINGS_MODEL=nomic-embed-text
# RAGAS_EMBEDDINGS_PROVIDER=openai  # Note: Ollama not supported for RAGAS, use OpenAI/HuggingFace
# RAGAS_EMBEDDING_MODEL=accounts/fireworks/models/nomic-embed-text-v1

# === Configuration 6: Mixed (Ollama for main, HuggingFace for RAGAS) ===
# EMBEDDINGS_PROVIDER=ollama
# EMBEDDINGS_MODEL=nomic-embed-text
# RAGAS_EMBEDDINGS_PROVIDER=huggingface
# RAGAS_EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# HUGGINGFACE_DEVICE=cpu
# HUGGINGFACE_NORMALIZE_EMBEDDINGS=true

# ============================================
# Configuration for deployment on Render.com with Groq
# ============================================
# Рекомендуемая конфигурация для деплоя на Render.com (работает без VPN)
# Подробная инструкция: docs/deployment-render.md

# === Groq LLM (быстро, бесплатно, работает на Render без VPN) ===
# OPENAI_BASE_URL=https://api.groq.com/openai/v1
# LLM_MODEL=llama-3.1-8b-instant
# OPENAI_API_KEY=your_groq_api_key_here
# LLM_PROVIDER=openai

# === HuggingFace Embeddings (бесплатно, работает на Render) ===
# EMBEDDINGS_PROVIDER=huggingface
# EMBEDDINGS_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# RAGAS_EMBEDDINGS_PROVIDER=huggingface
# RAGAS_EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# HUGGINGFACE_DEVICE=cpu
# HUGGINGFACE_NORMALIZE_EMBEDDINGS=true

# === Дополнительные настройки ===
# SYSTEM_ROLE=банковский ассистент
# CONTEXT_TURNS=8
# RETRIEVER_K=4
# DATA_PATH=@data
# LOG_LEVEL=INFO
# SHOW_SOURCES=false